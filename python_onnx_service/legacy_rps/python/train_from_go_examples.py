import json
import argparse
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os

# --- PyTorch Model Definitions ---

class PolicyNetwork(nn.Module):
    """MLP Policy Network matching the Go implementation structure."""
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_size, output_size)
        # Use LogSoftmax for CrossEntropyLoss compatibility, or keep linear for MSE/KLDivergence
        # Using LogSoftmax as CrossEntropyLoss is common for policy targets
        self.log_softmax = nn.LogSoftmax(dim=1)

    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        x = self.log_softmax(x)
        return x

class ValueNetwork(nn.Module):
    """MLP Value Network matching the Go implementation structure."""
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_size, 1) # Output is a single value
        self.tanh = nn.Tanh() # Output range [-1, 1] matching typical value range

    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        x = self.tanh(x)
        return x

# --- PyTorch Dataset --- #

class GoExamplesDataset(Dataset):
    """Loads training examples from the JSON generated by the Go script."""
    def __init__(self, json_path):
        print(f"Loading examples from {json_path}...")
        start_time = time.time()
        try:
            with open(json_path, 'r') as f:
                # The JSON is an array of objects like:
                # { "BoardState": [...], "PolicyTarget": [...], "ValueTarget": ... }
                self.examples = json.load(f)
        except FileNotFoundError:
            print(f"Error: File not found at {json_path}")
            self.examples = []
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON from {json_path}: {e}")
            self.examples = []
        
        load_time = time.time() - start_time
        print(f"Loaded {len(self.examples)} examples in {load_time:.2f} seconds.")
        
        if not self.examples:
            raise ValueError("No valid examples loaded from JSON file.")

        # Validate structure of the first example (simple check)
        first_example = self.examples[0]
        self.input_size = len(first_example.get('BoardState', []))
        self.policy_output_size = len(first_example.get('PolicyTarget', []))
        print(f"Inferred Input Size: {self.input_size}")
        print(f"Inferred Policy Output Size: {self.policy_output_size}")
        if self.input_size == 0 or self.policy_output_size == 0 or 'ValueTarget' not in first_example:
             raise ValueError("Invalid example structure in JSON file.")

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        example = self.examples[idx]
        board_state = torch.tensor(example['BoardState'], dtype=torch.float32)
        policy_target = torch.tensor(example['PolicyTarget'], dtype=torch.float32)
        value_target = torch.tensor([example['ValueTarget']], dtype=torch.float32) # Ensure value target is a tensor
        return board_state, policy_target, value_target

# --- Training Loop --- #

def train(args):
    """Loads data, trains policy and value networks, saves models."""

    # Setup device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load data
    try:
        dataset = GoExamplesDataset(args.examples_path)
    except (ValueError, FileNotFoundError) as e:
        print(f"Error loading dataset: {e}")
        return
        
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)

    # Initialize models
    policy_net = PolicyNetwork(dataset.input_size, args.hidden_size, dataset.policy_output_size).to(device)
    value_net = ValueNetwork(dataset.input_size, args.hidden_size).to(device)

    # Optimizers
    policy_optimizer = optim.Adam(policy_net.parameters(), lr=args.learning_rate)
    value_optimizer = optim.Adam(value_net.parameters(), lr=args.learning_rate)

    # Loss functions
    # CrossEntropyLoss expects raw logits from model and class indices as target
    # OR log-probabilities from model (LogSoftmax) and probabilities as target.
    # Since our targets are probabilities, we use LogSoftmax in the model and KLDivergenceLoss here,
    # or CrossEntropyLoss if targets were integer indices.
    # Let's use KL Divergence Loss which compares probability distributions.
    # policy_criterion = nn.CrossEntropyLoss()
    policy_criterion = nn.KLDivLoss(reduction='batchmean') # KL divergence for policy probability distribution
    value_criterion = nn.MSELoss() # Mean Squared Error for value regression

    print(f"Starting training for {args.epochs} epochs...")
    total_start_time = time.time()

    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        total_policy_loss = 0.0
        total_value_loss = 0.0
        num_batches = 0

        for i, (states, policy_targets, value_targets) in enumerate(dataloader):
            states = states.to(device)
            policy_targets = policy_targets.to(device)
            value_targets = value_targets.to(device)

            # --- Train Policy Network ---
            policy_optimizer.zero_grad()
            policy_log_probs = policy_net(states)
            # KLDivLoss expects log-probabilities as input and probabilities as target.
            policy_loss = policy_criterion(policy_log_probs, policy_targets)
            policy_loss.backward()
            policy_optimizer.step()
            total_policy_loss += policy_loss.item()

            # --- Train Value Network ---
            value_optimizer.zero_grad()
            predicted_values = value_net(states)
            value_loss = value_criterion(predicted_values, value_targets)
            value_loss.backward()
            value_optimizer.step()
            total_value_loss += value_loss.item()
            
            num_batches += 1
            
            # Print progress occasionally
            if (i + 1) % 100 == 0:
                 print(f'  Epoch [{epoch+1}/{args.epochs}], Step [{i+1}/{len(dataloader)}], ' 
                       f'Policy Loss: {policy_loss.item():.4f}, Value Loss: {value_loss.item():.4f}')

        avg_policy_loss = total_policy_loss / num_batches
        avg_value_loss = total_value_loss / num_batches
        epoch_time = time.time() - epoch_start_time
        print(f"Epoch {epoch+1}/{args.epochs} Summary:")
        print(f"  Avg Policy Loss: {avg_policy_loss:.4f}")
        print(f"  Avg Value Loss:  {avg_value_loss:.4f}")
        print(f"  Time: {epoch_time:.2f}s")

    total_training_time = time.time() - total_start_time
    print(f"Training finished in {total_training_time:.2f} seconds.")

    # --- Save Models ---
    print(f"Saving trained models...")
    try:
        torch.save(policy_net.state_dict(), args.output_policy_path)
        print(f"  Policy model saved to {args.output_policy_path}")
        torch.save(value_net.state_dict(), args.output_value_path)
        print(f"  Value model saved to {args.output_value_path}")
    except Exception as e:
        print(f"Error saving models: {e}")

# --- Main Execution --- #

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train PyTorch Policy/Value networks from Go self-play examples.")

    parser.add_argument("--examples-path", type=str, required=True, 
                        help="Path to the input JSON file containing training examples.")
    parser.add_argument("--hidden-size", type=int, default=64, 
                        help="Number of hidden neurons in the networks.")
    parser.add_argument("--epochs", type=int, default=10, 
                        help="Number of training epochs.")
    parser.add_argument("--batch-size", type=int, default=64, 
                        help="Training batch size.")
    parser.add_argument("--learning-rate", type=float, default=0.001, 
                        help="Optimizer learning rate.")
    parser.add_argument("--output-policy-path", type=str, default="output/pytorch_policy_h64.pt", 
                        help="Path to save the trained policy model (.pt).")
    parser.add_argument("--output-value-path", type=str, default="output/pytorch_value_h64.pt", 
                        help="Path to save the trained value model (.pt).")

    args = parser.parse_args()

    # Ensure output directory exists (relative to script location)
    # Very basic check, assumes paths are relative like 'output/model.pt'
    if '/' in args.output_policy_path:
        os.makedirs(os.path.dirname(args.output_policy_path), exist_ok=True)
    if '/' in args.output_value_path:
         os.makedirs(os.path.dirname(args.output_value_path), exist_ok=True)

    train(args) 